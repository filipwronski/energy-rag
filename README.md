# Energy RAG - Protocol Document Search System

An advanced RAG (Retrieval Augmented Generation) system for semantic search files, featuring:

- üöÄ **OpenRouter API** - `text-embedding-3-small` embeddings (1536-dim)
- üîÑ **Query Expansion** - Hybrid query variant generation (LLM + rules)
- üéØ **Reciprocal Rank Fusion** - Intelligent result aggregation
- üíæ **SQLite Cache** - 80-90% API cost reduction
- üìÑ **OCR** - PDF to Markdown conversion with EasyOCR
- ü§ñ **Q&A System** - Natural language answers powered by DeepSeek V3.2

## Table of Contents

- [Key Features](#key-features)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [Usage](#usage)
- [How It Works](#how-it-works)
- [Costs](#costs)
- [Configuration](#configuration)
- [FAQ](#faq)

## Key Features

### RAG System
- ‚úÖ **5 query variants** - original + 2 LLM + 2 rule-based (synonyms, word order)
- ‚úÖ **RRF Aggregation** - Fuses 50 results (5 variants √ó 10) ‚Üí top 20 best matches
- ‚úÖ **Embedding Cache** - SQLite with automatic hit rate tracking
- ‚úÖ **Optimized chunks** - 512 characters with 50 overlap for better precision
- ‚úÖ **Cost tracking** - Full API cost monitoring

### Q&A System
- ‚úÖ **Natural language answers** - DeepSeek V3.2 generates answers based on RAG results
- ‚úÖ **Smart filtering** - Only high-quality documents (RRF score > 0.04)
- ‚úÖ **Up to 20 contextual documents** - Adaptive result count (typically 5-15)
- ‚úÖ **Source citations** - Automatic protocol numbers and dates
- ‚úÖ **Interactive mode** - Conversational interface for asking questions
- ‚úÖ **Low cost** - DeepSeek V3.2: $0.27/$1.10 per 1M tokens (75x cheaper than Claude)

### PDF OCR
- ‚úÖ PDF ‚Üí Markdown conversion with EasyOCR
- ‚úÖ OCR for Polish and English
- ‚úÖ Automatic detection: text PDF vs scanned images
- ‚úÖ High-quality text recognition

## Quick Start

### 1. Start Qdrant (Docker)

```bash
docker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant
```

### 2. Configure API Key

```bash
# Copy template
cp .env.example .env

# Edit .env and add your OpenRouter API key
nano .env
```

Get your API key at [OpenRouter](https://openrouter.ai/keys)

### 3. Build Index

```bash
# If you have PDFs: Convert to Markdown first
python pdf_to_markdown_easyocr.py

# Build Qdrant index
python scripts/build_index.py
```

**One-time cost**: ~$0.01-0.02 for ~4,500 chunks

### 4. Ask Questions

**Q&A System (natural language answers):**
```bash
python scripts/ask.py "what renovations were done at Bonifacego 66?"
```

**Classic Search (retrieve fragments):**
```bash
python scripts/search.py "employee matters"
```

## Installation

### Requirements

- Docker
- 4GB RAM minimum (8GB recommended)
- 2GB free disk space

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure API Key

Create a `.env` file with your OpenRouter API key:

```bash
cp .env.example .env
```

Edit `.env`:
```
OPEN_ROUTER_API_KEY=sk-or-v1-your_actual_key_here
```

**How to get API key:**
1. Register at [OpenRouter](https://openrouter.ai/)
2. Go to [Keys](https://openrouter.ai/keys)
3. Create new API key
4. Copy key to `.env` file

**‚ö†Ô∏è SECURITY:**
- **NEVER** commit `.env` to git (already in `.gitignore`)
- If key leaks, rotate it immediately at https://openrouter.ai/keys
- Don't share your key publicly

### 3. Prepare Input Files

#### Option A: Download PDFs (if available)

```bash
python scripts/download_pdfs.py
```

#### Option B: Add Your Own PDFs

Place PDF files in directories:
- `input/` - main protocols
- `input-sp/` - estate protocols (optional)

**Note:** Directories `input/`, `input-sp/`, `output/` and `output-sp/` are git-ignored. You must generate them locally.

### 4. Build Index

```bash
# Start Qdrant (Docker)
docker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant

# In new terminal: Convert PDFs to Markdown (if you have PDFs)
python pdf_to_markdown_easyocr.py

# Build Qdrant index
python scripts/build_index.py
```

## Usage

### Q&A System (Natural Language Answers)

**Basic mode:**
```bash
python scripts/ask.py "what repairs were done at Bonifacego Street?"
```

**Output example:**
```
======================================================================
Q&A System
Powered by RAG + DeepSeek V3.2
======================================================================

Question: what repairs were done at Bonifacego Street?

======================================================================
ANSWER:
======================================================================
Based on searched documents, the following repairs were conducted
at Bonifacego 66:

1. **Heating system renovation** (Protocol #15, 2024)
   - Radiator replacement in apartments
   - Cost: 45,000 PLN

2. **Roof repair** (Protocol #23, 2023)
   - Roof covering repair
   - Gutter replacement
   - Cost: 78,000 PLN

üìö Sources (20 documents):
  1. Protocol #15, Page 2 (Date: 19.08.-03.09.2024)
  2. Protocol #23, Page 1 (Date: 21.-28.06.2023)
  ...
======================================================================
```

**Interactive mode:**
```bash
python scripts/ask.py
```

Allows asking multiple questions in one session:
```
üí¨ Question: what decisions were made about waste shelters?
üí¨ Question: who was hired in 2023?
üí¨ Question: exit
```

**Options:**
- `--verbose` - detailed mode (show RAG statistics)
- `--no-sources` - run without displaying sources
- `--stats` - system statistics

### Classic Search (Retrieve Fragments)

**Basic search:**
```bash
python scripts/search.py "employee matters"
```

**Verbose mode:**
```bash
python scripts/search.py --verbose "employee matters"
```

Shows:
- Generated query variants (5 versions)
- RRF fusion statistics
- Cache hit rate
- Number of API calls

**Interactive mode:**
```bash
python scripts/search.py
```

**Available commands:**
- `--verbose` - toggle detailed mode
- `--stats` - show session statistics (cache, API calls)
- `exit` / `quit` - exit

### PDF to Markdown Conversion

```bash
# Download PDFs (optional)
python scripts/download_pdfs.py

# Convert PDF ‚Üí Markdown
python pdf_to_markdown_easyocr.py
```

**Process:**
1. PyMuPDF extracts pages as images
2. EasyOCR recognizes text (Polish + English)
3. Formats as Markdown with page headers
4. Saves to `output/`

**OCR Accuracy:**
- ‚úÖ Clean print: 95-99%
- ‚úÖ Scanned documents: 90-95%
- ‚ö†Ô∏è Handwritten: 60-80%

## How It Works

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        USER QUERY                               ‚îÇ
‚îÇ                   "employee matters"                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   QUERY EXPANSION                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ  Original    ‚îÇ  ‚îÇ LLM (GPT-4o) ‚îÇ  ‚îÇ Rule-Based   ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  Query       ‚îÇ  ‚îÇ 2 variants   ‚îÇ  ‚îÇ 2 variants   ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Output: 5 query variants                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    EMBEDDING (with Cache)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ For each variant:                                     ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  1. Check SQLite cache (SHA256 hash)                 ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  2. If miss ‚Üí OpenRouter API (text-embedding-3-small)‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  3. Store in cache for future reuse                  ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Output: 5 √ó 1536-dim vectors                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  VECTOR SEARCH (Qdrant)                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ For each variant vector:                              ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Query Qdrant collection (cosine similarity)        ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Retrieve top 10 chunks                             ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Total: 5 variants √ó 10 = 50 candidate chunks       ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              RECIPROCAL RANK FUSION (RRF)                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ Formula: RRF_score(d) = Œ£ 1/(k + rank(d))            ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ where k=60 (constant), rank = position (1-indexed)    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ                                                        ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ Process:                                               ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  1. Deduplicate chunks across variants                ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  2. Calculate RRF score for each unique chunk         ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  3. Sort by RRF score (descending)                    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  4. Filter by MIN_RRF_SCORE (0.04)                    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  5. Return top results (typically 5-15)               ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      FINAL RESULTS                              ‚îÇ
‚îÇ  ‚Ä¢ Top chunks with highest RRF scores (>0.04)                   ‚îÇ
‚îÇ  ‚Ä¢ Metadata: source, page, protocol number                      ‚îÇ
‚îÇ  ‚Ä¢ Optional: LLM-generated natural language answer              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Step-by-Step Process

#### 1. Query Expansion (5 variants)

**Goal:** Increase recall through different formulations of the same question.

| Method | Example | Generator |
|--------|----------|-----------|
| **Original** | "employee matters" | Original query |
| **LLM #1** | "employment issues" | GPT-4o-mini |
| **LLM #2** | "staffing concerns" | GPT-4o-mini |
| **Synonym** | "worker issue" | Synonym dictionary |
| **Word order** | "matters employee" | Word permutation |

#### 2. Embedding with Cache

```python
# For each variant:
query_hash = sha256(variant_text)

if cache.exists(query_hash):
    embedding = cache.get(query_hash)  # Cache HIT
else:
    embedding = openrouter.get_embedding(variant_text)  # API call
    cache.put(query_hash, embedding)   # Store in cache

# Result: 5 √ó 1536-dimensional vectors
```

**Cache Benefits:**
- 80-90% reduction in API calls (after ~50 queries)
- Instant retrieval for repeated queries
- SQLite - lightweight, zero setup

#### 3. Vector Search (Qdrant)

```python
for variant_embedding in variant_embeddings:
    results = qdrant.search(
        collection="energy_protocols",
        query_vector=variant_embedding,
        limit=10  # Top 10 per variant
    )
    all_results.append(results)

# Total candidates: 5 variants √ó 10 results = 50 chunks
```

#### 4. Reciprocal Rank Fusion

**Formula:**
```
RRF_score(chunk) = Œ£ (for all variants where chunk appeared)
                    1 / (60 + rank)

where:
- rank = chunk position in that variant's results (1-indexed)
- 60 = RRF constant from literature (balances precision/recall)
```

**Example:**
```
Chunk A appeared in:
- Variant 1: rank 2  ‚Üí 1/(60+2) = 0.0161
- Variant 3: rank 5  ‚Üí 1/(60+5) = 0.0154
- Variant 4: rank 1  ‚Üí 1/(60+1) = 0.0164
Total RRF score = 0.0479

Chunk B appeared in:
- Variant 2: rank 1  ‚Üí 1/(60+1) = 0.0164
Total RRF score = 0.0164

Ranking: Chunk A > Chunk B (more variants = higher score)
```

**RRF Advantages:**
- Chunks appearing in multiple variants get boosted
- No need to calibrate score thresholds
- Robust against outliers

#### 5. Result Filtering & Output

- Filter results by `MIN_RRF_SCORE` (default: 0.04)
- Typically returns 5-15 high-quality results
- Includes metadata: protocol number, page, date
- Optional: Generate natural language answer with LLM

## Costs

### API Costs (OpenRouter)

**Embedding Model:** `text-embedding-3-small` - $0.00002 per 1K tokens
**Query Expansion LLM:** `gpt-4o-mini` - $0.15/$0.60 per 1M tokens (input/output)
**Q&A LLM:** `deepseek/deepseek-chat` (DeepSeek V3.2) - $0.27/$1.10 per 1M tokens

#### One-Time Indexing
```
~4,500 chunks √ó 100 tokens/chunk = 450,000 tokens
Cost: (450,000 / 1,000) √ó $0.00002 = $0.009

Actual cost: $0.01-0.02
```

#### Per Query (Classic Search)
```
Components:
1. Query expansion (LLM):     ~$0.000025
2. Embeddings (5 variants):    ~$0.000002
Total (without cache):         ~$0.000027

With cache (80% hit rate):     ~$0.000025
```

#### Per Query (Q&A System)
```
Components:
1. RAG search (above):         ~$0.000025
2. DeepSeek V3.2 answer:       ~$0.000135  (500 tokens in + 500 out)
Total:                         ~$0.000160

Cost for 1000 Q&A queries: ~$0.16
```

**Model Comparison (Q&A):**
- DeepSeek V3.2: $0.000160/query ‚Üí **$0.16 per 1000 queries** ‚úÖ
- Claude 3.5 Sonnet: $0.012/query ‚Üí **$12 per 1000 queries** (75x more expensive!)
- GPT-4o: $0.0075/query ‚Üí **$7.50 per 1000 queries** (47x more expensive!)

#### Monthly Costs

```
Scenario A (search only):
  1,000 queries/month √ó $0.000025 = $0.025
  Annual cost: ~$0.30

Scenario B (Q&A only with DeepSeek):
  1,000 Q&A queries/month √ó $0.000160 = $0.16
  Annual cost: ~$1.92

Scenario C (mixed):
  500 search + 500 Q&A = (500 √ó $0.000025) + (500 √ó $0.000160) = $0.09
  Annual cost: ~$1.11
```

**Conclusion:** The system is extremely cheap to maintain! üéâ Even with DeepSeek Q&A, the cost is only ~$2/year for 1000 questions monthly!

### Cache Effectiveness

After ~50 queries:
```
Cache Hit Rate: 70-90%
API Calls Reduction: 80-90%
Cost Savings: ~$0.80 per 1,000 queries
```

**Cache Storage:**
```
~100 queries = ~1MB in SQLite
~1,000 queries = ~10MB
```

### Performance Metrics

| Metric | Cold Cache | Warm Cache |
|---------|------------|------------|
| **Query Time** | 1.2-1.5s | 0.8-1.0s |
| **API Calls** | 5-6 | 1-2 |
| **Cost** | $0.00003 | $0.000025 |

## Configuration

Key parameters in `rag/config.py`:

### Embeddings
```python
EMBEDDING_MODEL = "openai/text-embedding-3-small"
EMBEDDING_DIM = 1536
```

### Chunking
```python
MAX_CHUNK_SIZE = 512    # Reduced from 1000 for better precision
CHUNK_OVERLAP = 50      # Reduced from 100
```

### Query Expansion
```python
NUM_QUERY_VARIANTS = 5
NUM_LLM_VARIANTS = 2
NUM_RULE_VARIANTS = 2
```

### RRF
```python
RRF_K = 60                  # Standard constant
RESULTS_PER_VARIANT = 10    # Candidates per variant
DEFAULT_TOP_K = 20          # Maximum final results
MIN_RRF_SCORE = 0.04        # Minimum quality threshold
```

### Cache
```python
ENABLE_CACHE = True
CACHE_DB_PATH = "embedding_cache.db"
```

## FAQ

### Can I use a different embedding model?

Yes! Change in `rag/config.py`:
```python
EMBEDDING_MODEL = "openai/text-embedding-3-large"  # Larger model
EMBEDDING_DIM = 3072
```

**Note:** Requires reindexing the database.

### Can I use a different LLM for Q&A?

Yes! The Q&A system defaults to DeepSeek V3.2 (cheap and good), but you can change it:

**Option 1:** Change default model in `rag/qa_system.py`:
```python
def __init__(self, model: str = "anthropic/claude-3.5-sonnet"):  # Instead of deepseek
```

**Option 2:** Specify model during initialization:
```python
from rag.qa_system import ProtocolQASystem
qa = ProtocolQASystem(model="anthropic/claude-3.5-sonnet")
```

**Available models on OpenRouter:**
- `deepseek/deepseek-chat` - DeepSeek V3.2 (cheapest, good) ‚úÖ
- `google/gemini-2.0-flash-exp:free` - Gemini 2.0 Flash (free!)
- `anthropic/claude-3.5-sonnet` - Claude 3.5 Sonnet (best, expensive)
- `openai/gpt-4o` - GPT-4o (very good, expensive)
- Full list: https://openrouter.ai/models

### How do I adjust the quality threshold?

The system filters weak results using `MIN_RRF_SCORE` (default 0.04). Change in `rag/config.py`:

```python
MIN_RRF_SCORE = 0.04  # Default threshold
```

**Guidelines:**
- **0.02** - more results, may contain weak matches
- **0.04** - balanced (recommended) ‚úÖ
- **0.06** - only very good matches, fewer results
- **0.10** - extremely restrictive, only perfect matches

### How often should I clear the cache?

Never, unless:
- Cache > 100MB (check: `ls -lh embedding_cache.db`)
- Changing embedding model
- Testing cache hit rate from zero

### Does it work offline?

Not in the current version (requires OpenRouter API). For offline:
1. Restore `PolishEmbedder` (local model)
2. Disable LLM expansion (only rule-based)
3. Reindex with local model

### How do I rebuild the index?

If you add new markdown files to `output/`:

```bash
python scripts/build_index.py
```

**NOTE:** This will delete the current index and create a new one. Cache remains intact.

## Project Structure

```
energy-rag/
‚îú‚îÄ‚îÄ input/                          # PDF files for conversion
‚îú‚îÄ‚îÄ output/                         # Generated Markdown files
‚îú‚îÄ‚îÄ rag/                           # RAG module
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ openrouter_client.py       # OpenRouter API client
‚îÇ   ‚îú‚îÄ‚îÄ cache.py                   # SQLite cache for embeddings
‚îÇ   ‚îú‚îÄ‚îÄ openrouter_embedder.py     # Embedder with cache integration
‚îÇ   ‚îú‚îÄ‚îÄ query_expander.py          # Hybrid query expansion
‚îÇ   ‚îú‚îÄ‚îÄ rrf_aggregator.py          # Reciprocal Rank Fusion
‚îÇ   ‚îú‚îÄ‚îÄ enhanced_retriever.py      # Main orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ qa_system.py               # Q&A system with LLM
‚îÇ   ‚îî‚îÄ‚îÄ chunker.py                 # Document parsing and chunking
‚îú‚îÄ‚îÄ scripts/                       # User scripts
‚îÇ   ‚îú‚îÄ‚îÄ build_index.py             # Indexing with cost estimation
‚îÇ   ‚îú‚îÄ‚îÄ search.py                  # Enhanced CLI search
‚îÇ   ‚îú‚îÄ‚îÄ ask.py                     # Q&A system
‚îÇ   ‚îî‚îÄ‚îÄ download_pdfs.py           # PDF download
‚îú‚îÄ‚îÄ tests/                         # Tests
‚îÇ   ‚îî‚îÄ‚îÄ test_retrieval.py          # RAG test suite
‚îú‚îÄ‚îÄ .env                           # API keys (not in git)
‚îú‚îÄ‚îÄ embedding_cache.db             # SQLite cache (auto-generated)
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îî‚îÄ‚îÄ README.md                      # Documentation
```

## Troubleshooting

### Rate Limiting (429 errors)

**Symptom:** `Rate limited. Waiting 5s...`

**Solution:**
1. Increase `time.sleep(0.5)` ‚Üí `time.sleep(1.0)` in `rag/openrouter_client.py`
2. Reduce `batch_size` in `scripts/build_index.py` from 50 ‚Üí 20

### Cache growing too fast

**Symptom:** `embedding_cache.db > 100MB`

**Solution:**
```python
from rag.cache import EmbeddingCache
cache = EmbeddingCache()
cache.clear()  # Remove all entries
```

### LLM expansion failures

**Symptom:** `Warning: LLM expansion failed`

**Solution:**
- Check `OPEN_ROUTER_API_KEY` in `.env`
- Check API limits at [OpenRouter Dashboard](https://openrouter.ai/activity)
- System automatically falls back to rule-based expansion

### Slow queries (>3s)

**Symptom:** Consistent query time > 2s

**Solution:**
1. Reduce `RESULTS_PER_VARIANT` in `config.py` from 10 ‚Üí 5
2. Reduce `NUM_LLM_VARIANTS` from 2 ‚Üí 1
3. Wait for cache hit rate to increase (after ~50 queries)

### Too many weak results

**Symptom:** Results with low RRF scores (0.01-0.03), irrelevant documents

**Solution:**
Increase `MIN_RRF_SCORE` in `config.py`:
```python
MIN_RRF_SCORE = 0.06  # Instead of 0.04 (more restrictive)
```

### Too few results

**Symptom:** System returns only 2-3 results, even though other relevant documents exist

**Solution:**
1. Decrease `MIN_RRF_SCORE` in `config.py`:
   ```python
   MIN_RRF_SCORE = 0.02  # Instead of 0.04 (less restrictive)
   ```
2. Increase `RESULTS_PER_VARIANT`: 10 ‚Üí 15 (more candidates)

## Testing

### Run Test Suite

```bash
python tests/test_retrieval.py
```

**Tests:**
1. ‚úÖ Query expansion - variant generation
2. ‚úÖ RRF fusion - aggregation with mock data
3. ‚úÖ End-to-end search - full flow (requires Qdrant)
4. ‚úÖ Cache hit rate - cache effectiveness

## Roadmap

### Planned Features

- [ ] **Semantic Reranking** - 2-stage retrieval with cross-encoder
- [ ] **Query Classification** - filtering by protocol type
- [ ] **Highlight Variants** - showing which words from variants matched
- [ ] **A/B Testing** - comparison with previous system
- [ ] **Streaming Results** - progressive display for long results
- [ ] **Multi-language Support** - extension to other languages
- [ ] **Web UI** - graphical interface (Streamlit/Gradio)

### Possible Optimizations

- [ ] **Hybrid Search** - combination of vector + keyword (BM25)
- [ ] **Result Caching** - cache entire results (not just embeddings)
- [ ] **Batch Querying** - handle multiple queries simultaneously
- [ ] **Custom Synonyms** - learning from query logs
- [ ] **Feedback Loop** - implicit relevance feedback

## Contributing

Report bugs and feature requests through GitHub Issues.

Pull requests are welcome! üéâ

## License

Open source - use freely!

## Acknowledgments

Technologies used in this project:
- **OpenRouter** - unified LLM & embeddings API
- **Qdrant** - vector database
- **EasyOCR** - optical character recognition
- **PyMuPDF** - PDF processing
- **DeepSeek** - affordable high-quality LLM
- **Anthropic Claude** - code generation & planning

---

**Built with ‚ù§Ô∏è for better document search**
